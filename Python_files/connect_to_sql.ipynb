{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Connected to database: project_statistics\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "# Your connection string (update username, password, database as needed)\n",
    "connection_string = 'mysql+pymysql://root:Santquirze-19@127.0.0.1:3306/Project_Statistics'\n",
    "\n",
    "# Create the engine\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "# Test the connection\n",
    "with engine.connect() as connection:\n",
    "    result = connection.execute(text(\"SELECT DATABASE();\"))\n",
    "    db_name = result.scalar()\n",
    "    print(f\"✅ Connected to database: {db_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POPULATE THE TABLES OF THE SQL DATABASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Create SQLAlchemy engine for your MySQL database\n",
    "connection_string = 'mysql+pymysql://root:Santquirze-19@127.0.0.1:3306/Project_Statistics'\n",
    "engine = create_engine(connection_string)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTANT: LOAD DATA OF df_cleaned_merged_web_data_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['client_id', 'visitor_id', 'visit_id', 'process_step', 'date_time']\n"
     ]
    }
   ],
   "source": [
    "# Load the cleaned data file — make sure it reads all columns\n",
    "df = pd.read_csv('/Users/jon/Desktop/Ironhack/Unit 4 - Statistics & Probability/Project-Statistics/Databases/cleaned/df_cleaned_merged_web_data_pt.txt', index_col=False)\n",
    "\n",
    "# Confirm column names\n",
    "print(\"Columns:\", df.columns.tolist())  # Should show: ['client_id', 'visitor_id', 'visit_id', 'process_step', 'date_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(subset=['client_id', 'visitor_id', 'visit_id', 'date_time'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully uploaded to df_cleaned_merged_web_data\n"
     ]
    }
   ],
   "source": [
    "# Upload to the SQL table (must already exist!)\n",
    "df.to_sql('df_cleaned_merged_web_data', con=engine, if_exists='append', index=False)\n",
    "\n",
    "print(\"✅ Successfully uploaded to df_cleaned_merged_web_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTANT: LOAD DATA OF df_cleaned_experiment_clients_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['client_id', 'Variation']\n"
     ]
    }
   ],
   "source": [
    "# Load the cleaned data file — make sure it reads all columns\n",
    "df_1 = pd.read_csv('/Users/jon/Desktop/Ironhack/Unit 4 - Statistics & Probability/Project-Statistics/Databases/cleaned/df_cleaned_experiment_clients_data.txt', index_col=False)\n",
    "\n",
    "# Confirm column names\n",
    "print(\"Columns:\", df_1.columns.tolist()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully uploaded to df_experiment_clients_cleaned\n"
     ]
    }
   ],
   "source": [
    "# Upload to the SQL table (must already exist!)\n",
    "df_1.to_sql('df_experiment_clients_cleaned', con=engine, if_exists='append', index=False)\n",
    "\n",
    "print(\"✅ Successfully uploaded to df_experiment_clients_cleaned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTANT: LOAD DATA OF df_final_demo_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['client_id', 'client_tenure_year', 'client_tenure_month', 'client_age', 'gender', 'number_of_accounts_', 'balance', 'calls_past_6_months', 'logins_last_6_months']\n"
     ]
    }
   ],
   "source": [
    "# Load the cleaned data file — make sure it reads all columns\n",
    "df_2 = pd.read_csv('/Users/jon/Desktop/Ironhack/Unit 4 - Statistics & Probability/Project-Statistics/Databases/cleaned/df_final_demo_cleaned.txt', index_col=False)\n",
    "\n",
    "# Confirm column names\n",
    "print(\"Columns:\", df_2.columns.tolist()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully uploaded to df_final_demo_cleaned\n"
     ]
    }
   ],
   "source": [
    "df_2.to_sql('df_final_demo_cleaned', con=engine, if_exists='append', index=False)\n",
    "\n",
    "print(\"✅ Successfully uploaded to df_final_demo_cleaned\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
